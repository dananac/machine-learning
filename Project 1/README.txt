Dana Conley
CS 422 Machine Learning
Project 1 Write-Up


When completing Project 1, I implemented my decision tree functions recursively. Once the total entropy and the information gain for each feature are calculated, the feature with the highest information gain is chosen. Then, the samples that are on the branch with an entropy of 0 are removed from the training data. The tree is updated in the format [left, right, feature], where the feature assessed is added, and the terminating leaf is added in either the left or right spot, depending on the location of the leaf in the tree. In the spot that isn’t filled with a value, the function is then called again, passing in the new dataset. This repeats until the tree is complete and all samples have a designated location, or until the max depth is reached or information gain is equal to zero. I’m glad that I implemented this function recursively, because I didn’t need to repeat calculations for each depth of the tree. Instead, the function can call itself and repeat the process for as many times as needed. Implementing iteratively would likely have taken much longer to code, and the code would not have been as clean and readable.


When creating the random forest, the individual trees likely have such variance in their accuracy because each of the data samples vary slightly from each other, which can result in major changes for the development of the individual decision tree. Each individual decision tree has an accuracy that reflects these changes. The individual trees have a tendency to overfit the data by closely fitting the training data, which also reflects a lower accuracy for the individual tree. The variance could be reduced and accuracy potentially improved by having less features represented in the training data or by including even more samples in the training data.


It can be beneficial for the random forest to use an odd number of individual trees because it increases the chances of having an overall decision tree and accuracy represented by a majority of the individuals trees. Using an even number of individual trees makes it much less likely to have a majority, as the individual trees could demonstrate an even split between a decision tree and accuracy.


The aspects of python that I feel I need to work on the most would be working with dictionaries as well as general recursion. I find that I spend a lot of time fixing errors that occur with each recursion of a function. Additionally, I would like to become more familiar with working with classes and nodes, as I feel like it could’ve been helpful during my coding processes, however I didn’t feel comfortable enough in working with classes and chose to keep my trees organized with nested lists instead. I’ve coded in python for other classes, but it’s been a while, and I don’t know if I ever felt fully comfortable with the language. I am definitely starting to feel decently comfortable with python, but could still use some practice moving forward.